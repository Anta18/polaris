{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip -q install \"transformers>=4.44.0\" datasets bitsandbytes accelerate peft safetensors\n\nimport os, time, json, math, random\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Any\n\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments,\n    BitsAndBytesConfig, TrainerCallback\n)\nfrom peft import (\n    LoraConfig, get_peft_model, prepare_model_for_kbit_training,\n    PeftModel, TaskType\n)\n\nBASE_MODEL = os.getenv(\"BASE_MODEL\", \"Qwen/Qwen3-8B\")    # alt: \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\nOUTPUT_DIR = os.getenv(\"OUTPUT_DIR\", \"/kaggle/working/polaris-mn-qlora-qwen3-8b\")\nMAX_LEN     = int(os.getenv(\"MAX_LEN\", \"1536\"))   # 1536 to stay comfy on T4\nLR          = float(os.getenv(\"LR\", \"2e-4\"))\nEPOCHS      = float(os.getenv(\"EPOCHS\", \"1\"))\nBATCH_PER_DEV = int(os.getenv(\"BATCH_PER_DEV\", \"1\"))\nGRAD_ACC    = int(os.getenv(\"GRAD_ACC\", \"8\"))   \nWARMUP      = float(os.getenv(\"WARMUP\", \"0.03\"))\nLORA_R      = int(os.getenv(\"LORA_R\", \"16\"))\nLORA_ALPHA  = int(os.getenv(\"LORA_ALPHA\", \"32\"))\nLORA_DROPOUT= float(os.getenv(\"LORA_DROPOUT\", \"0.05\"))\nSAVE_MERGED = os.getenv(\"SAVE_MERGED\", \"0\") == \"1\"  \n\nSYSTEM_PROMPT = \"You are POLARIS, a precise, fair news summariser. Be neutral; cite key numbers/dates/actors.\"\n\nTRAIN_SAMPLES = int(os.getenv(\"TRAIN_SAMPLES\", \"4000\"))\nEVAL_SAMPLES  = int(os.getenv(\"EVAL_SAMPLES\",  \"400\"))\n\ndef load_qlora_base(model_name: str):\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n    )\n    tok = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n    if tok.pad_token is None:\n        tok.pad_token = tok.eos_token\n\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        quantization_config=bnb_config,\n        device_map=\"auto\",\n        low_cpu_mem_usage=True,\n    )\n    model.config.use_cache = False  \n    model = prepare_model_for_kbit_training(model)\n    lora_cfg = LoraConfig(\n        task_type=TaskType.CAUSAL_LM,\n        r=LORA_R, lora_alpha=LORA_ALPHA, lora_dropout=LORA_DROPOUT,\n        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n        bias=\"none\"\n    )\n    model = get_peft_model(model, lora_cfg)\n    return tok, model\n\ndef build_chat_strings(document: str, summary: str):\n    user = (\n        \"Summarise the following multi-article news package into a balanced digest. \"\n        \"State agreements and any conflicts; include key numbers/dates/actors.\\n\\nDOCUMENT:\\n\"\n        + document.strip()\n    )\n    msgs_full = [\n        {\"role\":\"system\",\"content\":SYSTEM_PROMPT},\n        {\"role\":\"user\",\"content\":user},\n        {\"role\":\"assistant\",\"content\":summary.strip()}\n    ]\n    msgs_prompt = [\n        {\"role\":\"system\",\"content\":SYSTEM_PROMPT},\n        {\"role\":\"user\",\"content\":user},\n        {\"role\":\"assistant\",\"content\":\"\"}\n    ]\n    return msgs_full, msgs_prompt\n\ndef tokenize_example(example, tokenizer: AutoTokenizer, max_len: int):\n    msgs_full, msgs_prompt = build_chat_strings(example[\"document\"], example[\"summary\"])\n    full_text   = tokenizer.apply_chat_template(msgs_full,   tokenize=False, add_generation_prompt=False)\n    prompt_text = tokenizer.apply_chat_template(msgs_prompt, tokenize=False, add_generation_prompt=False)\n\n    full_ids   = tokenizer(full_text,   truncation=True, max_length=max_len)\n    prompt_ids = tokenizer(prompt_text, truncation=True, max_length=max_len)\n\n    input_ids = full_ids[\"input_ids\"]\n    labels    = input_ids.copy()\n\n    prompt_len = len(prompt_ids[\"input_ids\"])\n    labels[:prompt_len] = [-100] * prompt_len  # mask prompt (only learn the answer)\n\n    return {\"input_ids\": input_ids, \"labels\": labels}\n\n@dataclass\nclass DataCollatorForCausalSupervised:\n    tokenizer: AutoTokenizer\n    pad_to_multiple_of: int = 8\n    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n        batch_input = self.tokenizer.pad(\n            {\"input_ids\": [f[\"input_ids\"] for f in features]},\n            padding=True, return_tensors=\"pt\",\n            pad_to_multiple_of=self.pad_to_multiple_of,\n        )\n        max_len = batch_input[\"input_ids\"].size(1)\n        batch_labels = []\n        for f in features:\n            l = f[\"labels\"]\n            if len(l) < max_len:\n                l = l + ([-100] * (max_len - len(l)))\n            batch_labels.append(l[:max_len])\n        batch_input[\"labels\"] = torch.tensor(batch_labels, dtype=torch.long)\n        return batch_input\n\nclass EtaLogger(TrainerCallback):\n    def __init__(self, log_path=\"/kaggle/working/metrics.jsonl\"):\n        self.start = None\n        self.max_steps = None\n        self.log_path = log_path\n    def on_train_begin(self, args, state, control, **kwargs):\n        self.start = time.time()\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        now = time.time()\n        steps_done = state.global_step or 0\n        if steps_done == 0: \n            return\n        if state.max_steps and state.max_steps > 0:\n            self.max_steps = state.max_steps\n        elapsed = now - self.start\n        rate = elapsed / max(1, steps_done)     # sec/step\n        if self.max_steps:\n            remaining = rate * (self.max_steps - steps_done)\n        else:\n            # fallback: estimate from logs.get(\"num_steps_per_epoch\", …) if present\n            remaining = float(\"nan\")\n        def hms(t):\n            if not (t==t) or math.isinf(t): return \"?\"\n            m, s = divmod(int(t), 60)\n            h, m = divmod(m, 60)\n            return f\"{h:02d}:{m:02d}:{s:02d}\"\n        msg = f\"[ETA] step {steps_done}/{self.max_steps or '?'} | {1.0/rate:.2f} steps/s | ETA {hms(remaining)} | elapsed {hms(elapsed)}\"\n        print(msg)\n        # also append to jsonl\n        rec = {\"time\": now, \"step\": steps_done, \"rate_steps_per_s\": (1.0/rate), \"eta_s\": None if not self.max_steps else remaining}\n        try:\n            with open(self.log_path, \"a\") as f:\n                f.write(json.dumps(rec) + \"\\n\")\n        except:\n            pass\n\nprint(\"[info] Loading Multi-News...\")\nds_train = load_dataset(\"alexfabbri/multi_news\", split=\"train\", revision=\"refs/convert/parquet\")\nds_eval  = load_dataset(\"alexfabbri/multi_news\", split=\"validation\", revision=\"refs/convert/parquet\")\n\ndef ok(e): return bool(e.get(\"document\",\"\").strip()) and bool(e.get(\"summary\",\"\").strip())\nds_train = ds_train.filter(ok)\nds_eval  = ds_eval.filter(ok)\n\nif TRAIN_SAMPLES > 0 and TRAIN_SAMPLES < len(ds_train):\n    ds_train = ds_train.select(range(TRAIN_SAMPLES))\nif EVAL_SAMPLES > 0 and EVAL_SAMPLES < len(ds_eval):\n    ds_eval = ds_eval.select(range(EVAL_SAMPLES))\n\nprint(\"[info] Examples:\", len(ds_train), \"train /\", len(ds_eval), \"eval\")\n\ntok, model = load_qlora_base(BASE_MODEL)\n\ndef map_fn(example): return tokenize_example(example, tok, MAX_LEN)\nds_train = ds_train.map(map_fn, remove_columns=ds_train.column_names, num_proc=2)\nds_eval  = ds_eval.map(map_fn,  remove_columns=ds_eval.column_names,  num_proc=2)\n\ncollator = DataCollatorForCausalSupervised(tok)\n\ntotal_train_tokens = sum(len(x[\"input_ids\"]) for x in ds_train)\nprint(f\"[info] tokenized train examples={len(ds_train)}  eval examples={len(ds_eval)}  ~train tokens={total_train_tokens:,}\")\n\n\nfrom inspect import signature, Parameter\n\nbf16_ok = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\nbase_kwargs = dict(\n    output_dir=OUTPUT_DIR,\n    learning_rate=LR,\n    num_train_epochs=EPOCHS,\n    per_device_train_batch_size=BATCH_PER_DEV,\n    per_device_eval_batch_size=max(1, BATCH_PER_DEV),\n    gradient_accumulation_steps=GRAD_ACC,\n    warmup_ratio=WARMUP,\n    lr_scheduler_type=\"cosine\",\n    logging_steps=20,\n    save_steps=200,\n    save_total_limit=2,\n    bf16=bf16_ok,\n    fp16=(not bf16_ok),\n    gradient_checkpointing=True,\n    ddp_find_unused_parameters=False,\n    report_to=\"none\",\n)\n\nsig = signature(TrainingArguments.__init__)\nparams = sig.parameters\n\nbase_kwargs[\"max_steps\"] = 100\nbase_kwargs[\"save_steps\"] = 50\nbase_kwargs[\"logging_steps\"] = 10\nif \"evaluation_strategy\" in params:\n    base_kwargs[\"evaluation_strategy\"] = \"steps\"\n    base_kwargs[\"eval_steps\"] = 200\nelif \"eval_strategy\" in params:\n    base_kwargs[\"eval_strategy\"] = \"steps\"\n    base_kwargs[\"eval_steps\"] = 200\n\nargs = TrainingArguments(**base_kwargs)\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=ds_train,\n    eval_dataset=ds_eval,\n    data_collator=collator,\n    tokenizer=tok,\n    callbacks=[EtaLogger()],\n)\n\ntrainer.train()\nif \"evaluation_strategy\" not in params and \"eval_strategy\" not in params:\n    print(trainer.evaluate())\n\n\ntrainer.model.save_pretrained(OUTPUT_DIR)  \ntok.save_pretrained(OUTPUT_DIR)\nprint(f\"[done] LoRA adapter + tokenizer saved to {OUTPUT_DIR}\")\n\nif SAVE_MERGED:\n    print(\"[info] Merging LoRA into base weights...\")\n    base = AutoModelForCausalLM.from_pretrained(\n        BASE_MODEL,\n        torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n        device_map=\"auto\"\n    )\n    peft_model = PeftModel.from_pretrained(base, OUTPUT_DIR)\n    merged = peft_model.merge_and_unload()\n    merged.save_pretrained(OUTPUT_DIR + \"-merged\", safe_serialization=True)\n    tok.save_pretrained(OUTPUT_DIR + \"-merged\")\n    print(f\"[done] Merged full model saved to {OUTPUT_DIR + '-merged'}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-08T04:59:19.287913Z","iopub.execute_input":"2025-11-08T04:59:19.288098Z","iopub.status.idle":"2025-11-08T08:11:41.145495Z","shell.execute_reply.started":"2025-11-08T04:59:19.288080Z","shell.execute_reply":"2025-11-08T08:11:41.144593Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m102.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"2025-11-08 05:00:46.617440: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762578046.801896      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762578046.853589      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"[info] Loading Multi-News...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"default/train/0000.parquet:   0%|          | 0.00/295M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4841966d8b8f46fd8a680a188eb247c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"default/train/0001.parquet:   0%|          | 0.00/28.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bc615722773478e961e297cf0d21fcb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"default/validation/0000.parquet:   0%|          | 0.00/39.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c06d75702724192925bc6be32c99df0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"default/test/0000.parquet:   0%|          | 0.00/40.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0efeb38a519c4eb2a8bbf5111bf3f8dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a57dd820a1c94c5294e96b2b8c11bf04"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52d44e5a034e4195b697e6e7d6ea6278"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9426fcb51a7488a84bc2619e922a8a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/44972 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95fe84a317d6447abf369dd78c7ae74c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/5622 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04254c22c216419db8f0eec995b3a550"}},"metadata":{}},{"name":"stdout","text":"[info] Examples: 4000 train / 400 eval\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad6eadb8975847b6b159dbe7ffc50940"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa459d51a6f54cf48275a8b1a0c3112e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c24f58562ee6425fb1c19220bcb53f31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2898ab802dfb461daeeaa7bcdf95c738"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/728 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab9a85e6511c49959a1b0071ebb5b72d"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c94fca2821e2496faf8ff4e8c3a95d9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a088d37518c4b70816bfd5181781151"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00005.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64395d899e3c42e294738cfa46de318a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00005.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ce6c04553d94864b6d418239195d8a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00005.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a247bb2c9af41ec876cc4ff64a6ce9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00005.safetensors:   0%|          | 0.00/3.19G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15ce1d9b1eba4dad8cb0b4918ed89108"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00005.safetensors:   0%|          | 0.00/1.24G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5921bb32c9d142cdac176d97a63a93d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03e2e9c20fe34df9aa2434f3255be621"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9695ffed71054e8ab2f285d20cf7c0a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/4000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75efbefef32148c2b109b098a295bd1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/400 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7784ac2890c467083f92783ded0a319"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_48/290894066.py:238: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"[info] tokenized train examples=4000  eval examples=400  ~train tokens=5,612,400\n","output_type":"stream"},{"name":"stderr","text":"You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 3:05:41, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"[ETA] step 10/100 | 0.01 steps/s | ETA 02:47:33 | elapsed 00:18:37\n[ETA] step 20/100 | 0.01 steps/s | ETA 02:26:53 | elapsed 00:36:43\n[ETA] step 30/100 | 0.01 steps/s | ETA 02:08:49 | elapsed 00:55:12\n[ETA] step 40/100 | 0.01 steps/s | ETA 01:51:29 | elapsed 01:14:19\n[ETA] step 50/100 | 0.01 steps/s | ETA 01:33:21 | elapsed 01:33:21\n[ETA] step 60/100 | 0.01 steps/s | ETA 01:14:30 | elapsed 01:51:46\n[ETA] step 70/100 | 0.01 steps/s | ETA 00:55:57 | elapsed 02:10:33\n[ETA] step 80/100 | 0.01 steps/s | ETA 00:37:14 | elapsed 02:28:58\n[ETA] step 90/100 | 0.01 steps/s | ETA 00:18:40 | elapsed 02:48:00\n[ETA] step 100/100 | 0.01 steps/s | ETA 00:00:00 | elapsed 03:07:28\n[ETA] step 100/100 | 0.01 steps/s | ETA 00:00:00 | elapsed 03:07:29\n[done] LoRA adapter + tokenizer saved to /kaggle/working/polaris-mn-qlora-qwen3-8b\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"def run_in_notebook(mode=\"multinews\", split=\"validation\", limit=3):\n    if mode == \"single\":\n        print(run_single_article_demo(\n            \"Sample Title\",\n            \"Officials said the agreement was reached after weeks of negotiation...\"\n        ))\n    elif mode == \"multinews\":\n        ds = load_multinews(split, limit)\n        for i in range(len(ds)):\n            out = run_multinews_topic_demo(ds[i], topic_hint=f\"Multi-News #{i}\")\n            print(f\"\\n=== Multi-News topic {i} ===\")\n            print(out[\"digest\"])\n            if out[\"gold_summary\"]:\n                print(\"\\n[REF]\", out[\"gold_summary\"])\n    else:\n        sample_articles = [\n            {\"title\":\"Climate pact announced\", \"text\":\"Officials said the agreement was reached after weeks...\"},\n            {\"title\":\"Talks lead to climate deal\", \"text\":\"Negotiators confirmed a deal on emissions reductions...\"},\n            {\"title\":\"Local sports team wins\", \"text\":\"The city celebrated as the team secured a victory...\"},\n        ]\n        outs = cluster_and_digest_raw_articles(sample_articles, topic_hint=\"Daily News\")\n        for o in outs:\n            print(f\"\\n=== Cluster {o['cluster_id']} ===\")\n            print(\"TITLES:\", o[\"titles\"])\n            print(\"DIGEST:\", o[\"digest\"])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"run_in_notebook(mode=\"multinews\", split=\"validation\", limit=2)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}